{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import telebot\n",
    "import fitz\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from DATA import TOKEN\n",
    "\n",
    "bot = telebot.TeleBot(TOKEN)\n",
    "\n",
    "texts = []\n",
    "flag = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "word_embeddings = {}    \n",
    "\n",
    "# Загрузка токенизатора и модели\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2') #22M parametrs\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L12-v2') #135M parametrs(работает так себе)\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L12-v2').to(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2') #278M parametrs\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-large-en-v1.5')  # 434M parametrs\n",
    "model = AutoModel.from_pretrained('Alibaba-NLP/gte-large-en-v1.5', trust_remote_code=True).to(device)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "model_cross_encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'DiTy/cross-encoder-russian-msmarco').to(device)\n",
    "tokenizer_cross_encoder = AutoTokenizer.from_pretrained('DiTy/cross-encoder-russian-msmarco')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e67ccb88cb756b1a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@bot.message_handler(commands=['check_texts'])\n",
    "def start(msg):\n",
    "    bot.send_message(msg.chat.id, str(len(texts)))\n",
    "\n",
    "@bot.message_handler(commands=['start'])\n",
    "def start(msg):\n",
    "    bot.send_message(msg.chat.id, f'''Привет, {msg.from_user.first_name}, это реализованный мною https://github.com/vakulenk0/RAG_tg_bot.git\\nRAG (Retrieval Augmented Generation) телеграм бот, приятного пользования!''')\n",
    "\n",
    "# @bot.message_handler(commands=['clear'])\n",
    "# def clear(msg):\n",
    "#     bot.delete_message(msg.chat.id, msg.id)\n",
    "\n",
    "# Обработчик для получения нескольких PDF-файлов\n",
    "@bot.message_handler(content_types=['document'])\n",
    "def handle_media_group(msg):\n",
    "    if msg.document.mime_type == 'application/pdf':\n",
    "        texts.append(process_pdf(msg.document))\n",
    "        bot.send_message(msg.chat.id, 'Файл обработан')\n",
    "    else:\n",
    "        bot.send_message(msg.chat.id, f\"Файл {msg.document.file_name} не является PDF и не будет обработан.\")\n",
    "\n",
    "\n",
    "def process_pdf(document):\n",
    "    # Скачивание файла\n",
    "    file_info = bot.get_file(document.file_id)\n",
    "    print(f'file_info: {file_info}')\n",
    "    downloaded_file = bot.download_file(file_info.file_path)\n",
    "    # print(f'downloaded_file: {downloaded_file}')\n",
    "\n",
    "    # Сохранение файла временно\n",
    "    file_name = document.file_name\n",
    "    with open(file_name, 'wb') as new_file:\n",
    "        new_file.write(downloaded_file)\n",
    "\n",
    "    # Извлечение текста из PDF\n",
    "    text = extract_text_from_pdf(file_name)\n",
    "    print(f'text: {text.replace('\\n', '')}')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    # Использование PyMuPDF для извлечения текста\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "@bot.message_handler(commands=['train'])\n",
    "def train(msg):\n",
    "    if len(texts):\n",
    "        bot.send_message(msg.chat.id, 'Модель обучается...')\n",
    "        \n",
    "        # Добавление PAD токена\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "        # Параметры для батчевой обработки\n",
    "        batch_size = 1  # Установите размер батча в зависимости от объема видеопамяти\n",
    "\n",
    "        # Функция для обработки одного батча\n",
    "        def process_batch(texts_batch):\n",
    "            # Токенизация\n",
    "            inputs = tokenizer(texts_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            # Вычисление эмбеддингов токенов\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Получение эмбеддингов слов\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            return {texts_batch[i]: last_hidden_state[i].mean(dim=0).cpu().numpy() for i in range(len(last_hidden_state))}\n",
    "\n",
    "        # Обработка всех текстов в батчах\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = process_batch(batch_texts)\n",
    "            word_embeddings.update(batch_embeddings)\n",
    "\n",
    "        bot.send_message(msg.chat.id, 'Модель обучилась, можно задавать вопросы')\n",
    "        flag = True\n",
    "    else:\n",
    "        bot.send_message(msg.chat.id, 'Нет доступных для анализа текстов, пожалуйста, добавьте их')\n",
    "\n",
    "\n",
    "@bot.message_handler(content_types=['text'])\n",
    "def search(msg):\n",
    "    if len(texts) and flag:\n",
    "        def cosine_similarity(A, B):\n",
    "            dot_product = np.dot(A, B)\n",
    "            norm_A = np.linalg.norm(A)\n",
    "            norm_B = np.linalg.norm(B)\n",
    "            return dot_product / (norm_A * norm_B)\n",
    "\n",
    "        def search_in_base(task):\n",
    "            inputs = tokenizer(task, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            embedding = last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "            text = [(cosine_similarity(embedding, word_embeddings[text]), text) for text in texts]\n",
    "            sort_text = sorted(text, key=lambda x: -x[0])\n",
    "            return sort_text[:10]\n",
    "        \n",
    "        \n",
    "\n",
    "        answers = [(ans[0], ans[1]) for ans in search_in_base(msg.text)]\n",
    "        pairs = [(msg.text, answer[1]) for answer in answers]\n",
    "\n",
    "        encoded_inputs = tokenizer_cross_encoder([q[0] for q in pairs], [ans[1] for ans in pairs],\n",
    "                                                 return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Перемещение данных на устройство\n",
    "        encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
    "\n",
    "        # Получение предсказаний от модели\n",
    "        with torch.no_grad():\n",
    "            outputs = model_cross_encoder(**encoded_inputs)\n",
    "\n",
    "        # Логиты (сырые выходные данные модели)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Преобразование логитов в вероятности (используйте сигмоид для бинарной классификации)\n",
    "        relevance_scores = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        # print('Сырые выходные данные модели семантического поиска: \\n\\n', relevance_scores)\n",
    "\n",
    "        # Сортировка по релевантности\n",
    "        ranked_candidates = sorted(zip([ans[1] for ans in answers], relevance_scores), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # Вывод результатов\n",
    "        for i, (candidate, score) in enumerate(ranked_candidates):\n",
    "            print(f\"Rank {i + 1}: {candidate[:150]} (Score: {score})\")\n",
    "\n",
    "        from huggingface_hub import InferenceClient\n",
    "\n",
    "        client = InferenceClient(\n",
    "            \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            token=\"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\",\n",
    "        )\n",
    "\n",
    "        response = client.chat_completion(\n",
    "            # messages=[{\"role\": \"user\", \"content\": f'''Что говорится в этом тексте: {ranked_candidates[0][0]} про {quest}?. Отвечай на русском языке. Ничего не придумывай, отвечай\n",
    "            # только на основе полученного текста! Если текст не содержит информации по моему запросу, то просто отвечай, что \"Текст не содержит информации по вашему запросу\" и ничего больше!!!\n",
    "            # Если запрос не соответсвтует тексту не расписывай его!!!'''}],\n",
    "            messages=[{\"role\": \"user\",\n",
    "                       \"content\": f'''Выдели информацию из этого текста: \"{ranked_candidates[0][0]}\" про {msg.text}; Бери информацию только из этого текста! Отвечай только на русском языке!'''}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "\n",
    "        # Содержание ответа\n",
    "        content = response.choices[0].message['content']\n",
    "        bot.send_message(msg.chat.id, content)\n",
    "    else:\n",
    "        if len(texts) <= 0:\n",
    "            bot.send_message(msg.chat.id, 'Сначала добавьте файлы(PDF), в которых я мог бы найти нужную для вас информацию')\n",
    "        else:\n",
    "            bot.send_message(msg.chat.id, 'Обучите модель командой /train')\n",
    "\n",
    "\n",
    "\n",
    "bot.polling(none_stop=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15644ff1663bb6e5",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
